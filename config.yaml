# environment and task
env:
  nu: 0.05
  num_actuators: 9
  num_sensors: 10
  burnin: 0
  exp_name: KS
  max_episode_steps: 1_000
  frame_skip: 1  # Sticky actions
  soft_action: True  # Interpolate the actions over frame_skip frames
  autoreg_action: 0.0  # Autoregressive action hyperparameter
  seed: 2253461632  # keep fixed for now
  auto_generate_seed: True  # If True, generate a random seed. If False, use the seed value specified above.

# collector
collector:
  total_frames: 500_000
  init_random_frames: 25_000
  frames_per_batch: 1_000
  collector_device: cpu
  env_per_collector: 1
  reset_at_each_iter: False

# replay buffer
replay_buffer:
  size: 1_000_000
  prb: False # use prioritized experience replay; the original TQC paper does not prioritized replay
  scratch_dir:

# optim
optim:
  utd_ratio: 1.0
  actuator_loss_weight: 0.0  # Weight hyperparameter of actuation size in reward function
  gamma: 0.99
  lr: 3.0e-4
  weight_decay: 0.0
  batch_size: 256
  target_update_polyak: 0.995
  alpha_init: 1.0
  adam_eps: 1.0e-8

# network
network:
  architecture: gru  # one of base, lstm, attention, buffer, gru
  actor_hidden_sizes: [256, 256]
  critic_hidden_sizes: [512, 512, 512]
  n_quantiles: 25
  n_nets: 5  # Number of critic nets
  top_quantiles_to_drop_per_net: 2
  activation: relu  # The original TQC paper uses ReLu
  default_policy_scale: 1.0
  scale_lb: 0.1
  device: cpu
  auto_detect_device: True  # If True, will use GPU when available or else fall back on device specified above
  lstm:
    hidden_size: 128
    final_net_sizes: [128, 128]
    preprocessing_mlp_sizes: [128]
    feature_size: 128
    critic_hidden_sizes: [512, 512]
    n_quantiles: 25
    n_nets: 5
  attention:
    actor_mlp_hidden_sizes: [128]
    critic_hidden_sizes: [128]
    n_quantiles: 25
    n_nets: 5  # Number of critic nets
    num_memories: 10
    size_memory: 32
    n_heads: 1  # Number of heads of self attention layer
    attention_mlp_depth: 2  # this is default value in deepmind paper
    actor_memory_key: actor_memory  # name of the key under which the actor memory is stored in tensordict
    critic_memory_key: critic_memory  # name of the key under which the critic memory is stored in tensordict
    initialise_random_memory: True  # If True, initialise memory entries from a unit Gaussian. If False, set to 0.
  buffer:
    buffer_observation_key: observation_buffer
    size: 10

# logging
logger:
  backend: wandb
  mode: offline
  eval_iter: 25_000
  project_name: KS_control_LSTM
  filename:

# Disable output subdirectory
hydra:
  output_subdir: config
