# environment and task
env:
  nu: 0.08
  num_actuators: 4
  num_sensors: 3
  burnin: 0
  exp_name: KS_TQC
  max_episode_steps: 2_000
  seed: 42

# collector
collector:
  total_frames: 4_000_000
  init_random_frames: 25_000
  frames_per_batch: 1_000
  collector_device: cpu
  env_per_collector: 1
  reset_at_each_iter: False

# replay buffer
replay_buffer:
  size: 1_000_000
  prb: False # use prioritized experience replay; the original TQC paper does not prioritized replay
  scratch_dir: ${env.exp_name}_${env.seed}

# optim
optim:
  utd_ratio: 1.0
  gamma: 0.99
  lr: 3.0e-4
  weight_decay: 0.0
  batch_size: 256
  target_update_polyak: 0.995
  alpha_init: 1.0
  adam_eps: 1.0e-8

# network
network:
  actor_hidden_sizes: [256, 256]
  critic_hidden_sizes: [512, 512, 512]
  n_quantiles: 25
  n_nets: 5  # Number of critic nets
  top_quantiles_to_drop_per_net: 2
  activation: relu  # The original TQC paper uses ReLu
  default_policy_scale: 1.0
  scale_lb: 0.1
  device: cpu

# logging
logger:
  backend: wandb
  mode: online
  eval_iter: 25_000
