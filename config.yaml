# environment and task
env:
  nu: 0.05
  num_actuators: 9
  num_sensors: 10
  burnin: 0
  exp_name: KS
  max_episode_steps: 1_000
  frame_skip: 1  # Sticky actions
  soft_action: True  # Interpolate the actions over frame_skip frames
  autoreg_action: 0.0  # Autoregressive action hyperparameter
  seed: 2253461632  # keep fixed for now

# collector
collector:
  total_frames: 500_000
  init_random_frames: 25_000
  frames_per_batch: 1_000
  collector_device: cpu
  env_per_collector: 1
  reset_at_each_iter: False

# replay buffer
replay_buffer:
  size: 1_000_000
  prb: False # use prioritized experience replay; the original TQC paper does not prioritized replay
  scratch_dir:

# optim
optim:
  utd_ratio: 1.0
  actuator_loss_weight: 0.0  # Weight hyperparameter of actuation size in reward function
  gamma: 0.99
  lr: 3.0e-4
  weight_decay: 0.0
  batch_size: 256
  target_update_polyak: 0.995
  alpha_init: 1.0
  adam_eps: 1.0e-8

# network
network:
  architecture: lstm  # one of base, lstm, attention
  actor_hidden_sizes: [256, 256]
  critic_hidden_sizes: [512, 512, 512]
  n_quantiles: 25
  n_nets: 5  # Number of critic nets
  top_quantiles_to_drop_per_net: 2
  activation: relu  # The original TQC paper uses ReLu
  default_policy_scale: 1.0
  scale_lb: 0.1
  device: cpu
  lstm:
    hidden_size: 512
    final_net_sizes: [256]
    preprocessing_mlp_sizes: [256, 256]
    feature_size: 64
  attention:
    num_memories: 10
    size_memory: 25
    n_heads: 5
    memory_key: memory  # name of the key under which the memory is stored in tensordict
    initialise_random_memory: False  # If True, initialise memory entries from a unit Gaussian. If False, set to 0.

# logging
logger:
  backend: wandb
  mode: online
  eval_iter: 25_000
  filename:

# Disable output subdirectory
hydra:
  output_subdir: config
